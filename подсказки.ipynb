{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Поляоная диаграмма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cluster_profile(grouped_data, n_clusters):\n",
    "    # Нормализуем сгруппированные данные, приводя их к масштабу 0-1.\n",
    "    scaler = preprocessing.MinMaxScaler()\n",
    "    grouped_data = pd.DataFrame(scaler.fit_transform(grouped_data), columns=grouped_data.columns)\n",
    "    # Создаём список признаков\n",
    "    features = grouped_data.columns\n",
    "    # Создаём пустую фигуру\n",
    "    fig = go.Figure()\n",
    "    # В цикле визуализируем полярную диаграмму для каждого кластера\n",
    "    for i in range(n_clusters):\n",
    "        # Создаём полярную диаграмму и добавляем её на общий график\n",
    "        fig.add_trace(go.Scatterpolar(\n",
    "            r=grouped_data.iloc[i].values, # радиусы\n",
    "            theta=features, # название засечек\n",
    "            fill='toself', # заливка многоугольника цветом\n",
    "            name=f'Cluster {i}', # название — номер кластера\n",
    "        ))\n",
    "    # Обновляем параметры фигуры\n",
    "    fig.update_layout(\n",
    "        showlegend=True, # отображение легенды\n",
    "        autosize=False, # устанавливаем свои размеры графика\n",
    "        width=800, # ширина (в пикселях)\n",
    "        height=800, # высота (в пикселях)\n",
    "    )\n",
    "    # Отображаем фигуру\n",
    "    fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "pca = decomposition.PCA(n_components=2)\n",
    "\n",
    "pipeline = Pipeline([(\"preprocessing\", scaler), (\"modelling\", pca)])\n",
    "pipeline.fit(rfm_table_cleaned)\n",
    "rfm_table_processed = pipeline.transform(rfm_table_cleaned)\n",
    "rfm_table_processed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Тепловая карта корреляции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Трехмерная визуализация признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(rfm_table_cleaned, \n",
    "        x='Frequency', \n",
    "        y='Recency', \n",
    "        z='Monetary',\n",
    "        width=1000,  \n",
    "        height=700,\n",
    "        title='Трехмерная визуализация признаков'\n",
    "        )\n",
    "\n",
    "fig.update_traces(marker=dict(size=3),\n",
    "              selector=dict(mode='markers'))\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ДОбавление процентелей в describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count_buy = df.groupby('Клиент')['Количество'].sum().reset_index()\n",
    "new_percentile = [0.25, 0.5, 0.75, 0.9, 0.99]\n",
    "df_count_buy.describe(percentiles=new_percentile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.percentile (а, д)\n",
    "\n",
    "#а: Массив значений\n",
    "#q: Процентили или последовательность процентилей для вычисления, которые должны быть в диапазоне от 0 до 100 включительно."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply по нескольким столбцам DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analog(StockCode, Quantity, CustomerID):\n",
    "    filtr = (data['StockCode']==StockCode) &\\\n",
    "        (data['Quantity']==Quantity*(-1)) &\\\n",
    "            (data['CustomerID']==CustomerID)\n",
    "    if len(data[filtr])!=0:\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "\n",
    "negative_quantity['Analog']=negative_quantity.apply(lambda x: get_analog(x.StockCode, x.Quantity, x.CustomerID), axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Поиск по регулярному выражению. Там, где совпало с шаблоном, выводится True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['LetterInCode'] = data['StockCode'].str.contains('^[a-zA-Z]+', regex=True)\n",
    "LetterInCode = data[data['LetterInCode']==True]['StockCode']\n",
    "LetterInCode.unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Как переименовать столбец dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_country = data.groupby('Country')['CustomerID'].count().reset_index()\n",
    "df_group_country.rename(columns={'CustomerID':'Count'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Географическая карта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(df_group_country, locations='Country', color='Count', locationmode='country names')\n",
    "fig.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Столбчатая диаграмма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "ax = sns.barplot(x='Дата', y='Количество', data=df_gr_data_filtr)\n",
    "plt.title('Зависимость количества проданных товаров по месяцам')\n",
    "plt.xlabel('Месяц', fontsize = 12)\n",
    "plt.ylabel('Количество проданных товаров', fontsize = 12)\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Словарик с русским переводом признаков:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_rus = {\n",
    "    'age':'возраст',\n",
    "    'job':'сфера занятости',\n",
    "    .......\n",
    "}\n",
    "\n",
    "pd.concat([pd.DataFrame(list(dict_to_rus.values()), index = list(dict_to_rus.keys()), columns = ['расшифровка']), \n",
    "           df.describe().T], join='inner',  axis = 1, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разбиение на количественные и категориальные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_colums = [s for s in df.columns if df[s].dtypes=='object']\n",
    "numeric_colums = [s for s in df.columns if df[s].dtypes!='object']\n",
    "\n",
    "sns.pairplot(df[numeric_colums])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Добавление единичного столбца в массив"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([np.ones(X.shape[0]).reshape(-1, 1), X])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание столбыатых диаграмм в цикле в разрезе бинарного признака"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in object_colums:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    df.groupby('deposit')[col].hist()\n",
    "    plt.title(\"Гистограмма признака <<{}>>\".format(dict_to_rus[col]))\n",
    "    plt.xlabel(dict_to_rus[col])\n",
    "    plt.legend(['Нет', 'Да'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(cat_features) #число категориальных признаков\n",
    "fig, axes = plt.subplots(n, 2, figsize=(15, 40)) #фигура+n*2 координатных плоскостей\n",
    "#Создаем цикл для всех признаков из списка категориальных признаков\n",
    "for i, feature in enumerate(cat_features):\n",
    "    #Строим количественную столбчатую для долевого соотношения каждой из категорий в данных\n",
    "    count_data = (data[feature].value_counts(normalize=True)\n",
    "                  .sort_values(ascending=False)\n",
    "                  .rename('percentage')\n",
    "                  .reset_index())\n",
    "    count_barplot = sns.barplot(data=count_data, x='index', y='percentage', ax=axes[i][0])\n",
    "    count_barplot.xaxis.set_tick_params(rotation=60)\n",
    "    #Строим столбчатую диаграмму доли людей зарабатывающих >50K (среднее по столбцу income) в зависимости от категории\n",
    "    mean_barplot = sns.barplot(data=data, x=feature, y='income', ax=axes[i][1])\n",
    "    mean_barplot.xaxis.set_tick_params(rotation=60)\n",
    "plt.tight_layout() #выравнивание графиков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Два гарфика рядом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "ax = sns.barplot(x='День недели', y='Количество', data=df_day)\n",
    "plt.title('Зависимость количества проданных товаров по дням недели')\n",
    "plt.xlabel('День недели', fontsize = 12)\n",
    "plt.ylabel('Количество проданных товаров', fontsize = 12)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "ax = sns.barplot(x='День недели', y='Сумма', data=df_day)\n",
    "plt.title('Зависимость суммы продаж дням недели')\n",
    "plt.xlabel('День недели', fontsize = 12)\n",
    "plt.ylabel('Сумма продаж', fontsize = 12)\n",
    "plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кодирование признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing  import LabelEncoder\n",
    "\n",
    "LE=LabelEncoder()\n",
    "data['Education'] = LE.fit_transform(data['Education'])\n",
    "data['Living_With'] = LE.fit_transform(data['Living_With'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "columns_to_change = ['pickup_day_of_week', 'geo_cluster', 'events']\n",
    "\n",
    "ohe = OneHotEncoder(drop='first')\n",
    "data_onehot = ohe.fit_transform(train_data[columns_to_change]).toarray()\n",
    "data_onehot = pd.DataFrame(data_onehot, columns=ohe.get_feature_names())\n",
    "data_onehot.head()\n",
    "\n",
    "train_data = pd.concat(\n",
    "    [train_data.reset_index(drop=True).drop(columns_to_change, axis=1), data_onehot], \n",
    "    axis=1\n",
    ")\n",
    "print('Shape of data: {}'.format(train_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Соортировка словаря по ключу (по значению = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correl = sorted(correl.items(), key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ПОиск выжных признаков методом SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "bestfeatures = SelectKBest(score_func=chi2, k='all')\n",
    "fit = bestfeatures.fit(X, y)\n",
    "df_scores = pd.DataFrame(fit.scores_)\n",
    "df_columns = pd.DataFrame(X.columns)\n",
    "\n",
    "feature_scores = pd.concat([df_columns, df_scores], axis=1)\n",
    "feature_scores.columns = ['Specs', 'Score']\n",
    "print(feature_scores.nlargest(10, 'Score').sort_values('Score', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_regression, k=3)\n",
    "selector.fit(X_train, y_train)\n",
    " \n",
    "selector.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нормализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем объект для min-max нормализации\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "#Вычисляем параметры для нормализации - min и max для каждого столбца\n",
    "scaler.fit(X_train)\n",
    "#Производим преобразование для каждой из выборок\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаем объект класса логистическая регрессия\n",
    "log_reg = linear_model.LogisticRegression(random_state=42, solver='sag')\n",
    "#Обучаем модель, минизируя logloss\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "#Делаем предсказание класса\n",
    "y_pred_train = log_reg.predict(X_train)\n",
    "y_pred_test = log_reg.predict(X_test)\n",
    "\n",
    "print(round(metrics.f1_score(y_train, y_pred_train),2))\n",
    "print(round(metrics.f1_score(y_test, y_pred_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Поиск самых коррелированных признаков с целевой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Вычисляем модуль корреляции\n",
    "corr_with_target = boston_data.corr()['MEDV'].abs().sort_values()\n",
    "#Удаляем корреляцию целевой переменной с самой собой\n",
    "corr_with_target = corr_with_target.drop('MEDV')\n",
    "#Строим столбчатую диаграмму корреляций\n",
    "fig, ax = plt.subplots(figsize=(10, 5)) #фигура+координатная плоскость\n",
    "ax.bar(corr_with_target.index, corr_with_target.values) #столбчатая диаграмма\n",
    "ax.set_title('Correlations with target') #название графика\n",
    "ax.set_xlabel('Feature') #название оси x\n",
    "ax.set_ylabel('Сorrelation coefficient'); #название оси y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Линейная регрессия (метод наименьших квадратов)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "#Создаём объект класса LinearRegression\n",
    "lr_lstat = linear_model.LinearRegression()\n",
    "#Обучаем модель — ищем параметры по МНК\n",
    "lr_lstat.fit(X, y)\n",
    " \n",
    "print('w0: {}'.format(lr_lstat.intercept_)) #свободный член w0\n",
    "print('w1: {}'.format(lr_lstat.coef_)) #остальные параметры модели w1, w2, ..., wm\n",
    "\n",
    "#Предсказываем медианную цену для всех участков из набора данных\n",
    "y_predict = lr_lstat.predict(X)\n",
    "#Строим визуализацию\n",
    "plot_regression_2d(X, y, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Составляем таблицу из признаков и их коэффициентов\n",
    "w_df = pd.DataFrame({'Features': features, 'Coefficients': lr_full .coef_})\n",
    "#Составляем строку таблицы со свободным членом\n",
    "intercept_df =pd.DataFrame({'Features': ['INTERCEPT'], 'Coefficients': lr_full .intercept_})\n",
    "coef_df = pd.concat([w_df, intercept_df], ignore_index=True)\n",
    "display(coef_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрики в Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean_absolute_error() — расчёт MAE;<br/>\n",
    "mean_square_error() — расчёт MSE;<br/>\n",
    "mean_absolute_percentage_error() — расчёт MAPE;<br/>\n",
    "r2_score() — расчёт коэффициента детерминации ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Рассчитываем MAE\n",
    "print('MAE score: {:.3f} thou. $'.format(metrics.mean_absolute_error(y, y_predict_lstat)))\n",
    "#Рассчитываем RMSE\n",
    "print('RMSE score: {:.3f} thou. $'.format(np.sqrt(metrics.mean_squared_error(y, y_predict_lstat))))\n",
    "#Рассчитываем MAPE\n",
    "print('MAPE score: {:.3f} %'.format(metrics.mean_absolute_percentage_error(y, y_predict_lstat) * 100))\n",
    "#Рассчитываем коэффициент детерминации\n",
    "print('R2 score: {:.3f}'.format(metrics.r2_score(y, y_predict_lstat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Стохастичский градиентный спуск для линейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём объект класса линейной регрессии с SGD\n",
    "sgd_lr_lstat = linear_model.SGDRegressor(random_state=42)\n",
    "#Обучаем модель — ищем параметры по методу SGD\n",
    "sgd_lr_lstat.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!! Как мы уже говорили ранее, при использовании градиентного спуска и его модификаций очень важно масштабировать данные с помощью нормализации или стандартизации. Иначе алгоритм теряется в таком растянутом пространстве из-за неравномерных градиентов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Стандартизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    " \n",
    "#Инициализируем стандартизатор StandardScaler\n",
    "scaler = preprocessing.StandardScaler()\n",
    "#Производим стандартизацию\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "#Составляем DataFrame из результата\n",
    "X_scaled = pd.DataFrame(X_scaled, columns=features)\n",
    "X_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По приведённой таблице можно выделить примерные области применения каждого из методов:<br/>\n",
    "<br/>\n",
    "Если стоит задача одноразового обучения на всех данных, которые есть, и признаков немного (меньше 1 000), наш выбор — LinearRegression, так как МНК обеспечивает простое решение и гарантированную сходимость.<br/>\n",
    "Если стоит задача непрерывного обучения модели в процессе её эксплуатации или количество признаков очень велико, наш выбор — SGDRegressor с возможностью корректировки параметров на новых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Разбивка на тестовую и обучающую выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Разделяем выборку на тренировочную и тестовую в соотношении 70/30\n",
    "#Устанавливаем random_state для воспроизводимости результатов \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)\n",
    "#Выводим результирующие размеры таблиц\n",
    "print('Train:', X_train.shape, y_train.shape)\n",
    "print('Test:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Создание полиноииальных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём генератор полиномиальных признаков\n",
    "poly = preprocessing.PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly.fit(X_train)\n",
    "#Генерируем полиномиальные признаки для тренировочной выборки\n",
    "X_train_poly = poly.transform(X_train)\n",
    "#Генерируем полиномиальные признаки для тестовой выборки\n",
    "X_test_poly = poly.transform(X_test)\n",
    "#Выводим результирующие размерности таблиц\n",
    "print(X_train_poly.shape)\n",
    "print(X_test_poly.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем, созданные признаки можно скормить модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём объект класса LinearRegression\n",
    "lr_model_poly = linear_model.LinearRegression()\n",
    "#Обучаем модель по МНК\n",
    "lr_model_poly.fit(X_train_poly, y_train)\n",
    "#Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = lr_model_poly.predict(X_train_poly)\n",
    "#Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = lr_model_poly.predict(X_test_poly)\n",
    " \n",
    "#Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Регуляризация методом LAsso (L1 регуляризация)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "lasso_lr_poly = linear_model.Lasso(alpha=0.1)\n",
    "#Обучаем модель\n",
    "lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "#Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "#Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "#Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Регуляризация методом Ridge (L2 регуляризация)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём объект класса линейной регрессии с L2-регуляризацией\n",
    "ridge_lr_poly = linear_model.Ridge(alpha=10)\n",
    "#Обучаем модель\n",
    "ridge_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "#Делаем предсказание для тренировочной выборки\n",
    "y_train_predict_poly = ridge_lr_poly.predict(X_train_scaled_poly)\n",
    "#Делаем предсказание для тестовой выборки\n",
    "y_test_predict_poly = ridge_lr_poly.predict(X_test_scaled_poly)\n",
    "#Рассчитываем коэффициент детерминации для двух выборок\n",
    "print(\"Train R^2: {:.3f}\".format(metrics.r2_score(y_train, y_train_predict_poly)))\n",
    "print(\"Test R^2: {:.3f}\".format(metrics.r2_score(y_test, y_test_predict_poly)))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Подбор параметра альфа для L1 регуляризации в цикле"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём список из 20 возможных значений от 0.001 до 1\n",
    "alpha_list = np.linspace(0.001, 1, 20)\n",
    "#Создаём пустые списки, в которые будем добавлять результаты \n",
    "train_scores = []\n",
    "test_scores = []\n",
    "for alpha in alpha_list:\n",
    "    #Создаём объект класса линейной регрессии с L1-регуляризацией\n",
    "    lasso_lr_poly = linear_model.Lasso(alpha=alpha, max_iter=10000)\n",
    "    #Обучаем модель\n",
    "    lasso_lr_poly.fit(X_train_scaled_poly, y_train)\n",
    "    #Делаем предсказание для тренировочной выборки\n",
    "    y_train_predict_poly = lasso_lr_poly.predict(X_train_scaled_poly)\n",
    "    #Делаем предсказание для тестовой выборки\n",
    "    y_test_predict_poly = lasso_lr_poly.predict(X_test_scaled_poly)\n",
    "    #Рассчитываем коэффициенты детерминации для двух выборок и добавляем их в списки\n",
    "    train_scores.append(metrics.r2_score(y_train, y_train_predict_poly))\n",
    "    test_scores.append(metrics.r2_score(y_test, y_test_predict_poly))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Визуализируем изменение R^2 в зависимости от alpha\n",
    "fig, ax = plt.subplots(figsize=(12, 4)) #фигура + координатная плоскость\n",
    "ax.plot(alpha_list, train_scores, label='Train') #линейный график для тренировочной выборки\n",
    "ax.plot(alpha_list, test_scores, label='Test') #линейный график для тестовой выборки\n",
    "ax.set_xlabel('Alpha') #название оси абсцисс\n",
    "ax.set_ylabel('R^2') #название оси ординат\n",
    "ax.set_xticks(alpha_list) #метки по оси абсцисс\n",
    "ax.xaxis.set_tick_params(rotation=45) #поворот меток на оси абсцисс\n",
    "ax.legend(); #отображение легенды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Логистическая регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model #линейные модели\n",
    "\n",
    "#Создаём объект класса LogisticRegression\n",
    "log_reg_2d = linear_model.LogisticRegression(random_state=42)\n",
    "#Обучаем модель, минимизируя logloss\n",
    "log_reg_2d.fit(X, y)\n",
    "#Выводим результирующие коэффициенты\n",
    "print('w0: {}'.format(log_reg_2d.intercept_)) #свободный член w0\n",
    "print('w1, w2: {}'.format(log_reg_2d.coef_)) #остальные параметры модели w1, w2, ..., wm\n",
    "\n",
    "# w0: [-8.24898965]\n",
    "# w1, w2: [[0.03779275 0.0875742 ]]\n",
    "\n",
    "#Делаем предсказание вероятностей:\n",
    "y_new_proba_predict = log_reg_2d.predict_proba(x_new)\n",
    "print('Predicted probabilities: {}'.format(np.round(y_new_proba_predict, 2)))\n",
    "\n",
    "y_new_predict = log_reg_2d.predict(x_new)\n",
    "print('Predicted class: {}'.format(y_new_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Функция для визуализации модели\n",
    "def plot_probabilities_2d(X, y, model):\n",
    "    #Генерируем координатную сетку из всех возможных значений для признаков\n",
    "    #Glucose изменяется от x1_min = 44 до x2_max = 199, \n",
    "    #BMI — от x2_min = 18.2 до x2_max = 67.1\n",
    "    #Результат работы функции — два массива xx1 и xx2, которые образуют координатную сетку\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.arange(X.iloc[:, 0].min()-1, X.iloc[:, 0].max()+1, 0.1),\n",
    "        np.arange(X.iloc[:, 1].min()-1, X.iloc[:, 1].max()+1, 0.1)\n",
    "    )\n",
    "    #Вытягиваем каждый из массивов в вектор-столбец — reshape(-1, 1)\n",
    "    #Объединяем два столбца в таблицу с помощью hstack\n",
    "    X_net = np.hstack([xx1.reshape(-1, 1), xx2.reshape(-1, 1)])\n",
    "    #Предсказываем вероятность для всех точек на координатной сетке\n",
    "    #Нам нужна только вероятность класса 1\n",
    "    probs = model.predict_proba(X_net)[:, 1]\n",
    "    #Переводим столбец из вероятностей в размер координатной сетки\n",
    "    probs = probs.reshape(xx1.shape)\n",
    "    #Создаём фигуру и координатную плоскость\n",
    "    fig, ax = plt.subplots(figsize = (10, 5))\n",
    "    #Рисуем тепловую карту вероятностей\n",
    "    contour = ax.contourf(xx1, xx2, probs, 100, cmap='bwr')\n",
    "    #Рисуем разделяющую плоскость — линию, где вероятность равна 0.5\n",
    "    bound = ax.contour(xx1, xx2, probs, [0.5], linewidths=2, colors='black');\n",
    "    #Добавляем цветовую панель \n",
    "    colorbar = fig.colorbar(contour)\n",
    "    #Накладываем поверх тепловой карты диаграмму рассеяния\n",
    "    sns.scatterplot(x=X.iloc[:, 0], y=X.iloc[:, 1], hue=y, palette='seismic', ax=ax)\n",
    "    #Даём графику название\n",
    "    ax.set_title('Scatter Plot with Decision Boundary');\n",
    "    #Смещаем легенду в верхний левый угол вне графика\n",
    "    ax.legend(bbox_to_anchor=(-0.05, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрики для задачи классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "confusion_matrix() — расчёт матрицы ошибок,<br/>\n",
    "accuracy_score() — расчёт accuracy,<br/>\n",
    "precision_score() — расчёт precision,<br/>\n",
    "recall_score() — расчёт recall,<br/>\n",
    "f1_score() — расчёт -меры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "#Рассчитываем accuracy\n",
    "print('Accuracy: {:.2f}'.format(metrics.accuracy_score(y, y_pred2)))\n",
    "#Рассчитываем precision\n",
    "print('Precision: {:.2f}'.format(metrics.precision_score(y, y_pred2)))\n",
    "#Рассчитываем recall\n",
    "print('Recall: {:.2f}'.format(metrics.recall_score(y, y_pred2)))\n",
    "#Рассчитываем F1-меру\n",
    "print('F1 score: {:.2f}'.format(metrics.f1_score(y, y_pred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Мультиклассовая классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Создаём объект класса LogisticRegression\n",
    "log_reg = linear_model.LogisticRegression(\n",
    "    multi_class='multinomial', #мультиклассовая классификация\n",
    "    max_iter=1000, #количество итераций, выделенных на сходимость\n",
    "    random_state=42 #генерация случайных чисел\n",
    ")\n",
    " \n",
    "#Обучаем модель \n",
    "log_reg.fit(X_dummies, y)\n",
    "#Делаем предсказание вероятностей\n",
    "y_pred_proba = np.round(log_reg.predict_proba(X_dummies), 2)\n",
    "#Делаем предсказание класса\n",
    "y_pred = log_reg.predict(X_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Деревья решений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree #модели деревьев решения\n",
    "\n",
    "#Создаём объект класса DecisionTreeClassifier\n",
    "dt_clf_2d = tree.DecisionTreeClassifier(\n",
    "    criterion='entropy', #критерий информативности \n",
    "    max_depth=3, #максимальная глубина\n",
    "    random_state=42 #генератор случайных чисел\n",
    ")\n",
    "#Обучаем дерево решений по алгоритму CART\n",
    "dt_clf_2d.fit(X, y)\n",
    "\n",
    "#Создаём фигуру для визуализации графа\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "#Строим граф дерева решений\n",
    "tree.plot_tree(\n",
    "    dt_clf_2d, #объект обученного дерева\n",
    "    feature_names=X.columns, #наименования факторов\n",
    "    class_names=[\"0 - Not diabetic\", \"1 - Diabetic\"], #имена классов\n",
    "    filled=True, #расцветка графа\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "график разделяющей поверхности дерева решений для 2-х признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_probabilities_2d(X, y, model):\n",
    "    #Генерируем координатную сетку из всех возможных значений для признаков\n",
    "    #Glucose изменяется от 40 до 200, BMI — от 10 до 80\n",
    "    #Результат работы функции — два массива xx1 и xx2, которые образуют координатную сетку\n",
    "    xx1, xx2 = np.meshgrid(\n",
    "        np.arange(40, 200, 0.1),\n",
    "        np.arange(10, 80, 0.1)\n",
    "    )\n",
    "    #Вытягиваем каждый из массивов в вектор-столбец — reshape(-1, 1)\n",
    "    #Объединяем два столбца в таблицу с помощью hstack\n",
    "    X_net = np.hstack([xx1.reshape(-1, 1), xx2.reshape(-1, 1)])\n",
    "    #Предсказываем вероятность для всех точек на координатной сетке\n",
    "    #Нам нужна только вероятность класса 1\n",
    "    probs = model.predict_proba(X_net)[:, 1]\n",
    "    #Переводим столбец из вероятностей в размер координатной сетки\n",
    "    probs = probs.reshape(xx1.shape)\n",
    "    #Создаём фигуру и координатную плоскость\n",
    "    fig, ax = plt.subplots(figsize = (10, 5))\n",
    "    #Рисуем тепловую карту вероятностей\n",
    "    contour = ax.contourf(xx1, xx2, probs, 100, cmap='bwr')\n",
    "    #Рисуем разделяющую плоскость — линию, где вероятность равна 0.5\n",
    "    bound = ax.contour(xx1, xx2, probs, [0.5], linewidths=2, colors='black');\n",
    "    #Добавляем цветовую панель \n",
    "    colorbar = fig.colorbar(contour)\n",
    "    #Накладываем поверх тепловой карты диаграмму рассеяния\n",
    "    sns.scatterplot(data=X, x='Glucose', y='BMI', hue=y, palette='seismic', ax=ax)\n",
    "    #Даём графику название\n",
    "    ax.set_title('Scatter Plot with Decision Boundary');\n",
    "    #Смещаем легенду в верхний левый угол вне графика\n",
    "    ax.legend(bbox_to_anchor=(-0.05, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Делаем предсказание класса для всего набора данных\n",
    "y_pred = dt_clf_2d.predict(X)\n",
    "#Выводим отчёт о метриках классификации\n",
    "print(metrics.classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### СЛУЧАЙНЫЙ ЛЕС В SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "\n",
    "#Создаём объект класса RandomForestClassifier\n",
    "rf_clf_2d = ensemble.RandomForestClassifier(\n",
    "    n_estimators=500, #число деревьев\n",
    "    criterion='entropy', #критерий эффективности\n",
    "    max_depth=3, #максимальная глубина дерева\n",
    "    max_features='sqrt', #число признаков из метода случайных подпространств\n",
    "    random_state=42 #генератор случайных чисел\n",
    ")\n",
    "#Обучаем модель \n",
    "rf_clf_2d.fit(X, y)\n",
    " \n",
    "#Делаем предсказание класса\n",
    "y_pred = rf_clf_2d.predict(X)\n",
    "#Выводим отчёт о метриках\n",
    "print(metrics.classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Диаграмма важности признаков на основании деревьев из случайного леса:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(13, 5)) #фигура + координатная плоскость\n",
    "feature = X.columns #признаки\n",
    "feature_importances = rf.feature_importances_ #важность признаков\n",
    "#Строим столбчатую диаграмму\n",
    "sns.barplot(x=feature, y=feature_importances, ax=ax);\n",
    "#Добавляем подпись графику, осям абсцисс и ординат\n",
    "ax.set_title('Bar plot feature importances')\n",
    "ax.set_xlabel('Features')\n",
    "ax.set_ylabel('Importances');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Кластеризация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Кластеризация методом k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем нужный модуль k-means-кластеризации\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# инициализируем алгоритм, при желании задаём разные параметры для алгоритма\n",
    "k_means = KMeans(n_clusters=2, init='k-means++', n_init=10, random_state=42)\n",
    "X = df[[\"x1\", \"x2\", \"x3\"]]\n",
    "# обучаем модель на данных, передав матрицу наблюдений X\n",
    "k_means.fit(X)\n",
    "# получаем результаты кластеризации (список меток, к какому кластеру относится каждый объект из X)\n",
    "labels = k_means.labels_\n",
    "\n",
    "X_new = df2[[\"x1\", \"x2\", \"x3\"]]\n",
    "k_means.predict(X_new)\n",
    "\n",
    "#визуализируем результаты. Параметр c принимает вектор с номерами классов для группировки объектов по цветам \n",
    "sns.scatterplot(df.Attack, df.Defense, c=predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метод локтя\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# функция, которая принимает количество кластеров для k-means и матрицу с признаками объектов и возвращает инерцию \n",
    "def get_inertia(cluster_num, X):\n",
    "# инициализируем алгоритм кластеризации\n",
    "    k_means =  KMeans(n_clusters=cluster_num, random_state=42)\n",
    "# запускаем алгоритм k-means\n",
    "    k_means.fit(X)\n",
    "# находим значение инерции\n",
    "    inertia = k_means.inertia_\n",
    "# возвращаем значение инерции\n",
    "    return inertia\n",
    "\n",
    "# создаём пустой список для значений инерции\n",
    "inertia = []\n",
    "# итерируемся по разным размерам кластеров (от 1 до 9) и сохраняем значение инерции для каждого кластера\n",
    "for cluster_num in range(1, 10):\n",
    "# сохраняем значения\n",
    "    inertia.append(get_inertia(cluster_num, X))\n",
    "\n",
    "# визуализируем, как менялась инерция в зависимости от количества кластеров\n",
    "# задаём названия осям x и y\n",
    "plt.xlabel(\"cluster\", fontsize=12)\n",
    "plt.ylabel(\"inertia\", fontsize=12)\n",
    "# рисуем изменение инерции\n",
    "plt.plot([i for i in range(1, 10)], inertia, 'xb-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Коэффициент силуэта"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем метрику силуэта\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# напишем функцию, как и при подсчёте метода локтя\n",
    "def get_silhouette(cluster_num, X):\n",
    "    k_means =  KMeans(n_clusters=cluster_num, random_state=42)\n",
    "    k_means.fit(X)\n",
    "# подсчитаем метрику силуэта, передав данные и то, к каким кластерам относятся объекты\n",
    "    silhouette = silhouette_score(X, k_means.labels_)\n",
    "    return silhouette\n",
    "\n",
    "silhouette = []\n",
    "for clust_num in range(2, 10):\n",
    "    silhouette.append(get_silhouette(clust_num, X))\n",
    "\n",
    "# визуализируем коэффициенты силуэта для разного количества кластеров\n",
    "plt.xlabel(\"cluster\", fontsize=12)\n",
    "plt.ylabel(\"silhouette\", fontsize=12)\n",
    "plt.plot([i for i in range(2, 10)], silhouette, 'xb-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Метрики однородности кластеризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем подсчёт метрики однородности кластеров\n",
    "from sklearn.metrics.cluster import homogeneity_score\n",
    "\n",
    "# передаём предсказанную информацию (к какому кластеру относятся объекты датасета) и правильные ответы\n",
    "print(homogeneity_score(labels_true=[0, 0, 1, 1], labels_pred=[0, 0, 1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем метрику полноты\n",
    "from sklearn.metrics.cluster import completeness_score\n",
    "\n",
    "# передаём предсказанную информацию (к какому кластеру относятся объекты датасета) и правильные ответы, подсчитываем метрику\n",
    "completeness_score(labels_true=[0, 0, 1, 1], labels_pred=[0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем из библиотеки sklearn подсчёт V-меры\n",
    "from sklearn.metrics import v_measure_score\n",
    "\n",
    "# посчитаем V-меру для кластеров с покемонами\n",
    "print(v_measure_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k3))\n",
    ">1.0\n",
    "\n",
    "print(v_measure_score(labels_true=df.RealClusters, labels_pred=df.Clusters_k4))\n",
    ">0.9070246789753754"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем из библиотеки sklearn подсчёт индекса Рэнда\n",
    "from sklearn.metrics.cluster import rand_score\n",
    "# передаём в rand_score размеченные и предсказанные данные\n",
    "rand_score([0, 0, 1, 1], [0, 0, 1, 1])\n",
    "1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EM-кластеризация (для кластеризации документов по разным категориям, основываясь на тегах, заголовках или содержимом документа, нахождении опуходи на МРТ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем библиотеки numpy и sklearn\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gm_clustering = GaussianMixture(n_components=3, random_state=42)\n",
    "\n",
    "# обучаем модель \n",
    "gm_clustering.fit(X)\n",
    "\n",
    "# для матрицы X получаем предсказания, к какому кластеру принадлежат объекты\n",
    "gm_prediction = gm_clustering.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Спектральная кластеризация (для сегментации изображений)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# вызываем из sklearn SpectralClustering \n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "\n",
    "# запускаем кластеризацию, задав количество кластеров, равное 3 \n",
    "spectral_clustering = SpectralClustering(n_clusters=3, random_state=42)\n",
    "\n",
    "spectral_clustering.fit(df[['Attack', 'Defense']])\n",
    "# получаем результаты кластеризации\n",
    "spectral_predictions = spectral_clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN (кластеризация на основе плотности)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно использовать DBSCAN для разработки системы рекомендаций в магазине. Если мы точно не знаем, на какие группы можно разделить пользователей, то на основе того, что покупают люди, можно провести DBSCAN-кластеризацию.\n",
    "\n",
    "Данный алгоритм успешно справляется с поиском выбросов в данных. Рассмотренные ранее алгоритмы кластеризации, в отличие от DBSCAN, не выделяют выбросы в отдельные объекты.\n",
    "\n",
    "Часто DBSCAN применяют для кластеризации геоданных, так как он может выделять данные сложной формы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем DBSCAN-кластеризацию\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#запускаем кластеризацию на наших данных\n",
    "clustering = DBSCAN(eps=3, min_samples=3).fit(df[['Attack', 'Defense']])\n",
    "\n",
    "# Далее можно визуализировать результаты, как мы делали это с алгоритмом k-means\n",
    "sns.scatterplot(df.Attack, df.Defense, c=clustering.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Понижение размерности. Метод главных компонент (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# из модуля decomposition библиотеки sklearn импортируем класс PCA\n",
    "from sklearn.decomposition import PCA\n",
    "# создаём объект класса PCA\n",
    "# n_components — задаём количество компонентов для проведения трансформации\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "# обучаем модель на данных X\n",
    "pca.fit(X)\n",
    "# применяем уменьшение размерности к матрице X\n",
    "pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем класс TSNE из модуля manifold библиотеки sklearn\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# создаём объект класса TSNE\n",
    "# n_components — размерность нового пространства\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=500, random_state=42)\n",
    "# обучаем модель на данных X\n",
    "tsne.fit(X)\n",
    "# применяем уменьшение размерности к матрице X\n",
    "tsne.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Нахождение важных признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Обучение\\Skillfactory\\подсказки.ipynb Ячейка 94\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5/Skillfactory/%D0%BF%D0%BE%D0%B4%D1%81%D0%BA%D0%B0%D0%B7%D0%BA%D0%B8.ipynb#Y163sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m features \u001b[39m=\u001b[39m X_train\u001b[39m.\u001b[39mcolumns\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5/Skillfactory/%D0%BF%D0%BE%D0%B4%D1%81%D0%BA%D0%B0%D0%B7%D0%BA%D0%B8.ipynb#Y163sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m feature_import_dict \u001b[39m=\u001b[39m {}\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/%D0%9E%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5/Skillfactory/%D0%BF%D0%BE%D0%B4%D1%81%D0%BA%D0%B0%D0%B7%D0%BA%D0%B8.ipynb#Y163sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m importance \u001b[39m=\u001b[39m grb_reg\u001b[39m.\u001b[39mfeature_importances_\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "features = X_train.columns\n",
    "feature_import_dict = {}\n",
    "\n",
    "importance = grb_reg.feature_importances_\n",
    "\n",
    "for i in range(0,len(importance)):\n",
    "    feature_import_dict[features[i]] = importance[i]\n",
    "\n",
    "feature_import = pd.DataFrame.from_dict(list(feature_import_dict.items()))\n",
    "\n",
    "#Диаграмма\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "sns.barplot(data = feature_import, x = feature_import[0], y = feature_import[1])\n",
    "plt.xticks(rotation=45)\n",
    "plt.title('Значимость факторов для предсказания целевого признака');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0266ebab134052a9cf0b42e044d30c2b597e15661c65c66703e369e450494d6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
